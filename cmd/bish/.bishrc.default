# BISH_UPDATE_PROMPT gets called each time before bishop renders the prompt
# It should update the value of the $BISH_PROMPT environment variable
function BISH_UPDATE_PROMPT() {
  # BISH_PROMPT="bish> "
}

# The value of BISH_PROMPT is what gets rendered as the prompt
BISH_PROMPT="bish> "

# The minimum log level to log.
# Can be debug, info, warn, error, panic, fatal
BISH_LOG_LEVEL="info"

# Whether bishop should remove existing content in the log file when it starts
BISH_CLEAN_LOG_FILE=0

# Height of the assistant message box (help/completion/explanation) at the bottom of the screen
BISH_ASSISTANT_HEIGHT=3

# -------- Large Language Model Configuration --------
# - bishop invokes Large Language Models through OpenAI-compatible API
# - You can choose to use Ollama which runs LLM on your local machine
# - You can also use OpenAI or OpenRouter which runs LLM as a cloud service
# - Read the corresponding documentation of the model provider for config values below

# The "fast" model is used for auto suggestion.
# By default bishop uses qwen2.5 through Ollama as the fast model.
BISH_FAST_MODEL_API_KEY=ollama
BISH_FAST_MODEL_BASE_URL=http://localhost:11434/v1/
BISH_FAST_MODEL_ID=qwen2.5
BISH_FAST_MODEL_TEMPERATURE=0.1
BISH_FAST_MODEL_PARALLEL_TOOL_CALLS=true
BISH_FAST_MODEL_HEADERS='{}'

# The "slow" model is used for chat and agentic operations.
# By default bishop uses qwen2.5:32b through Ollama as the slow model.
BISH_SLOW_MODEL_API_KEY=ollama
BISH_SLOW_MODEL_BASE_URL=http://localhost:11434/v1/
BISH_SLOW_MODEL_ID=qwen2.5:32b
BISH_SLOW_MODEL_TEMPERATURE=0.1
BISH_FAST_MODEL_PARALLEL_TOOL_CALLS=true
BISH_SLOW_MODEL_HEADERS='{}'

# -------- RAG Configuration --------
# bishop uses Retrieval Augmented Generation (RAG) to get context from the environment and help give accurate results.
#
# Available context types include: 
# - system_info: current OS and architecture
# - working_directory: path of the current working directory
# - git_status: output from `git status`
# - history_concise: a concise version of command history
# - history_verbose: a verbose version of command history
#
# Retrieving more context will generally improve output quality at the cost of using more tokens and increased latency.

# A list of context to send to LLM along with agent chat messages.
BISH_CONTEXT_TYPES_FOR_AGENT=system_info,working_directory,git_status,history_verbose

# A list of context to send to LLM when predicting command with a partial prefix already entered by user
BISH_CONTEXT_TYPES_FOR_PREDICTION_WITH_PREFIX=system_info,working_directory,git_status,history_concise

# A list of context to send to LLM when predicting command with no prefix entered by user yet
BISH_CONTEXT_TYPES_FOR_PREDICTION_WITHOUT_PREFIX=system_info,working_directory,git_status,history_verbose

# A list of context to send to LLM when explaining command
BISH_CONTEXT_TYPES_FOR_EXPLANATION=system_info,working_directory

# How many recent commands to use in concise version of commmand history
BISH_CONTEXT_NUM_HISTORY_CONCISE=30

# How many recent commands to use in verbose version of commmand history
BISH_CONTEXT_NUM_HISTORY_VERBOSE=30

# -------- Agent Configuration --------
# Options below control behaviors of the chat agent.

# Whether confirmation prompts default to "yes" when Enter is pressed.
# When set to 1 or true, prompts display [Y/n] and Enter confirms.
# When set to 0 or false (default), prompts display [y/N] and Enter denies.
BISH_DEFAULT_TO_YES=0

# Size of the agent chat context window in LLM tokens.
# When the chat session exceeds this limit, only the most recent messages that 
# can fit in the window are kept.
BISH_AGENT_CONTEXT_WINDOW_TOKENS=32768

# A JSON array of regex patterns for bash commands that should be considered pre-approved
# Pre-approved commands will be executed without asking for confirmation
#
# This configuration works alongside the file-based authorization system:
# - Patterns defined here are always active (static pre-approval)
# - Additional patterns can be dynamically added to ~/.config/bish/authorized_commands
#   when you respond with 'a' (always) to command permission prompts
# - Both sources are checked when determining if a command should be auto-approved
#
# Example patterns that get automatically generated when using 'always allow':
# - "^ls.*" (matches any ls command)
# - "^git status.*" (matches git status with any flags)
# - "^npm install.*" (matches npm install commands)
# - "^docker run.*" (matches docker run commands)
BISH_AGENT_APPROVED_BASH_COMMAND_REGEX='[
  "^ls$",
  "^ls\\s+.*$",
  "^pwd$",
  "^pwd\\s+.*$",
  "^git\\s+status.*$",
  "^git\\s+log.*$",
  "^git\\s+diff.*$",
  "^git\\s+ls-files\\s+\\|\\s+grep\\s+.*$",
  "^echo\\s+.*$",
  "^cat\\s+.*$",
  "^grep\\s+.*$",
  "^find\\s+.*$",
  "^head\\s+.*$",
  "^tail\\s+.*$",
  "^wc\\s+.*$",
  "^which\\s+.*$",
  "^type\\s+.*$",
  "^file\\s+.*$"
]'

# A JSON object mapping macro names to their corresponding chat messages
BISH_AGENT_MACROS='{
  "gitdiff": "when inside of a git repository, review all staged and unstaged changes and write a concise summary",
  "gitpush": "when inside of a git repository, commit all staged and unstaged changes, and push them to remote; otherwise do nothing",
  "gitreview": "when inside of a git repository, review all staged and unstaged changes, and let me know if there are problems worth fixing; otherwise do nothing"
}'

# -------- Idle Summary Configuration --------
# When idle at the command prompt for this many seconds, bishop will summarize
# what you were doing based on recent commands. Set to 0 to disable.
BISH_IDLE_SUMMARY_TIMEOUT_SECONDS=60
